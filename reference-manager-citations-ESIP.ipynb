{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f15f333",
   "metadata": {},
   "source": [
    "# Reference Manager Citation Errors\n",
    "\n",
    "12-18-23 KV\n",
    "2-20-23 KV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "be3552a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "3ef7ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from where it's entered into CSV\n",
    "\n",
    "filepath = '/Users/kvrouwenvelder/Library/CloudStorage/OneDrive-AmericanGeophysicalUnion/Documents/Data/Reference-Manager-Work/reference-manager-analysis/'\n",
    "filename = 'reference-manager-data-v2-'\n",
    "reference_manager_filenames = ['BibTeX','repo-recommended-citation','Zotero-plugin','Zotero-wizard','BibDesk','Sciwheel-plugin',\n",
    "                               'SciWheel-wizard','CrossCite','Endnote-plugin','Endnote-wizard','Papers-plugin','Papers-wizard','Paperpile-plugin',\n",
    "                               'Paperpile-wizard','Mendeley-plugin','Mendeley-wizard','RefWorks-plugin','Refworks-wizard','Export-options']\n",
    "\n",
    "# import files\n",
    "bibtex = pd.read_csv(filepath+filename+reference_manager_filenames[0]+'.csv')\n",
    "repo_recommended_citation = pd.read_csv(filepath+filename+reference_manager_filenames[1]+'.csv')\n",
    "zotero_plugin = pd.read_csv(filepath+filename+reference_manager_filenames[2]+'.csv')\n",
    "zotero_wizard = pd.read_csv(filepath+filename+reference_manager_filenames[3]+'.csv')\n",
    "bib_desk = pd.read_csv(filepath+filename+reference_manager_filenames[4]+'.csv')\n",
    "sciwheel_plugin = pd.read_csv(filepath+filename+reference_manager_filenames[5]+'.csv')\n",
    "sciwheel_wizard = pd.read_csv(filepath+filename+reference_manager_filenames[6]+'.csv')\n",
    "crosscite = pd.read_csv(filepath+filename+reference_manager_filenames[7]+'.csv')\n",
    "endnote_plugin = pd.read_csv(filepath+filename+reference_manager_filenames[8]+'.csv')\n",
    "endnote_wizard = pd.read_csv(filepath+filename+reference_manager_filenames[9]+'.csv')\n",
    "papers_plugin = pd.read_csv(filepath+filename+reference_manager_filenames[10]+'.csv')\n",
    "papers_wizard = pd.read_csv(filepath+filename+reference_manager_filenames[11]+'.csv')\n",
    "paperpile_plugin = pd.read_csv(filepath+filename+reference_manager_filenames[12]+'.csv')\n",
    "paperpile_wizard = pd.read_csv(filepath+filename+reference_manager_filenames[13]+'.csv')\n",
    "mendeley_plugin = pd.read_csv(filepath+filename+reference_manager_filenames[14]+'.csv')\n",
    "mendeley_wizard = pd.read_csv(filepath+filename+reference_manager_filenames[15]+'.csv')\n",
    "refworks_plugin = pd.read_csv(filepath+filename+reference_manager_filenames[16]+'.csv')\n",
    "refworks_wizard = pd.read_csv(filepath+filename+reference_manager_filenames[17]+'.csv')\n",
    "export_options = pd.read_csv(filepath+filename+reference_manager_filenames[18]+'.csv')\n",
    "\n",
    "list_of_ref_managers = [bibtex,repo_recommended_citation,zotero_plugin,zotero_wizard,bib_desk,sciwheel_plugin,\n",
    "                       sciwheel_wizard,crosscite, endnote_plugin, endnote_wizard, papers_plugin,papers_wizard,paperpile_plugin,\n",
    "                       paperpile_wizard,mendeley_plugin,mendeley_wizard,refworks_plugin,refworks_wizard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ac6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "274c1b16",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "b7d350c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up input fields\n",
    "\n",
    "# Define export metadata fields for count_fields\n",
    "\n",
    "ex_cols = ['export-meta-type', 'export-meta-authors','export-meta-title','export-meta-date','export-meta-DOI',\n",
    "            'export-meta-repo','export-meta-version','export-meta-access-date']\n",
    "\n",
    "# Define import metadata fields for count_fields\n",
    "im_cols = ['import-meta-type', 'import-meta-authors','import-meta-title','import-meta-date','import-meta-DOI',\n",
    "            'import-meta-repo','import-meta-version','import-meta-access-date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e32c1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count correct, missing, incorrect metadata fields for each repository and reference manager (add cols to df)\n",
    "\n",
    "def count_fields(input_df,cols,col_type):\n",
    "    input_df['count_incorrect_'+col_type]=input_df[cols].sum(axis=1, numeric_only=True) # add a column to df with count \"incorrect\"\n",
    "    input_df['count_missing_'+col_type]= input_df[cols].isna().sum(axis=1, numeric_only=True) # add a column to df with count 'missing'\n",
    "    # The count correct is len(cols) - (# incorrect + # missing)\n",
    "    input_df['count_correct_'+col_type] = len(cols) - (input_df['count_incorrect_'+col_type]+input_df['count_missing_'+col_type])\n",
    "    return input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "2bbb46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count correct, missing, incorrect across all REFERENCE MANAGERS\n",
    "\n",
    "def sum_across_repos(list_of_dfs, cols, col_type): \n",
    "    # list_of_dfs is df names you wish to analyze; cols is list of columns to analyze; col_type is \"ex\" or \"im\"\n",
    "    \n",
    "    # init full sums\n",
    "    one_sum = np.zeros(3)\n",
    "    full_sums = np.tile(one_sum, (len(list_of_dfs), 1))\n",
    "    for i in range(len(full_sums)):\n",
    "        full_sums[i] = np.zeros(3)\n",
    "    \n",
    "\n",
    "    iter = 0 #start counter\n",
    "    \n",
    "    for df in list_of_dfs: # go through list of dfs\n",
    "        count_fields(df, cols, col_type)\n",
    "        sum_incorrect = np.sum(df['count_incorrect_' + col_type]) # this will be the first list element in return\n",
    "        sum_missing = np.sum(df['count_missing_' + col_type]) # This will be the second list element in return\n",
    "        sum_correct = np.sum(df['count_correct_' + col_type]) #this will be the 3rd list element in return\n",
    "        sums = [sum_incorrect, sum_missing, sum_correct]\n",
    "        full_sums[iter] = sums\n",
    "        iter += 1\n",
    "    \n",
    "    return full_sums\n",
    "    \n",
    "# print(full_sums[0][1]) # access an individual element in this list using list[rows][columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e178827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count correct, missing, incorrect across all REPOSITORIES\n",
    "\n",
    "def sum_across_refmans(list_of_dfs, cols, col_type): \n",
    "    # list_of_dfs is df names you wish to analyze; cols is list of columns to analyze; col_type is \"ex\" or \"im\"\n",
    "    \n",
    "    # init full sums df\n",
    "    # Number of rows will = # repos in this list\n",
    "    data = {str('sum_incorrect_'+col_type): np.zeros(len(repo_recommended_citation['Repository'])), #init w zeros\n",
    "        str('sum_missing_'+col_type): np.zeros(len(repo_recommended_citation['Repository'])),\n",
    "        str('sum_correct_'+col_type): np.zeros(len(repo_recommended_citation['Repository']))}\n",
    "\n",
    "\n",
    "    full_sums_df = pd.DataFrame(data) # make empty df \n",
    "    full_sums_df['Repository'] = repo_recommended_citation['Repository'] # Add repository column\n",
    "    \n",
    "    for df in list_of_dfs: # go through list of dfs\n",
    "        count_fields(df, cols, col_type) #returns that dataframe with the 3 sums added\n",
    "        \n",
    "        #add sum of each new column to existing dataframe\n",
    "        full_sums_df['sum_incorrect_'+col_type] += df['count_incorrect_'+col_type]\n",
    "        full_sums_df['sum_missing_'+col_type] += df['count_missing_'+col_type]\n",
    "        full_sums_df['sum_correct_'+col_type] += df['count_correct_'+col_type]\n",
    "    \n",
    "    return full_sums_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "733bd2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum across all import and export fields and return incorrect, missing, and correct sums\n",
    "\n",
    "def sum_across_fields(list_of_dfs): \n",
    "    \n",
    "    #initialize store_data\n",
    "\n",
    "    data = {'sum_incorrect': np.zeros(24), # set based on # of columns in endnote_plugin\n",
    "           'sum_missing': np.zeros(24),\n",
    "           'sum_correct': np.zeros(24)}\n",
    "\n",
    "    # run for first dataset in list_of_dfs to set correct indices for dataframe:\n",
    "\n",
    "    data['sum_incorrect'] = list_of_dfs[0].sum(axis=0, numeric_only=True) # sum incorrect\n",
    "    data['sum_missing'] = list_of_dfs[0].isna().sum(axis=0, numeric_only=True) # sum missing\n",
    "    data['sum_correct'].fill(len(list_of_dfs)*len(zotero_plugin['Repository'])) # sum correct; fill with total value possible for all fields! \n",
    "\n",
    "    # itera = 0\n",
    "\n",
    "    # iterate through list of dfs, excluding that first one\n",
    "\n",
    "    for df in list_of_dfs[1:]:\n",
    "        data['sum_incorrect'] += df.sum(axis=0, numeric_only=True) # sum incorrect\n",
    "        data['sum_missing'] += df.isna().sum(axis=0, numeric_only=True) # sum missing\n",
    "\n",
    "        # itera += 1\n",
    "        # print(itera) # just to check how far the loop makes it\n",
    "\n",
    "    # Calculate sum_correct as the total possible fields minus incorrect and missing sums\n",
    "    data['sum_correct'] = data['sum_correct'] - (data['sum_incorrect']+data['sum_missing'])\n",
    "\n",
    "    # create dataframe\n",
    "    store_data = pd.DataFrame(data)\n",
    "\n",
    "    # Note I checked this output using the export-meta-access-date column. sum_correct here is 25\n",
    "    # and looking at the source data, this matches the number of \"0\" codes we recorded!\n",
    "\n",
    "    return store_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926b668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c48d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a863c5c",
   "metadata": {},
   "source": [
    "# Repository recommended citation\n",
    "* How many repositories offer a recommended citation? \n",
    "* How many repositories offer a recommended citation that is correct when compared to DataCite metadata?\n",
    "* How many repositories define a citation style? \n",
    "* How many repositories offer an opportunity to change the citation style to what journal requires?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "10bb8ebd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Citation-style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Citation-style'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[288], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m count_fields(repo_recommended_citation, ex_cols,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# How many repositories specify citation style?\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m num_repo_citation_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(repo_recommended_citation[\u001b[43mrepo_recommended_citation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCitation-style\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAPA\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(num_repo_citation_style)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m repositories specify a citation style (APA).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create a stacked bar graph with correct, incorrect, missing fields for each repository\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m## Sort df by most correct fields\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Citation-style'"
     ]
    }
   ],
   "source": [
    "filename2 = 'repository-recommended-citation'\n",
    "\n",
    "# Count the correct , incorrect, missing fields - Export Only\n",
    "count_fields(repo_recommended_citation, ex_cols,'ex')\n",
    "\n",
    "# How many repositories specify citation style?\n",
    "\n",
    "num_repo_citation_style = len(repo_recommended_citation[repo_recommended_citation['Citation-style']=='APA'])\n",
    "print(str(num_repo_citation_style)+' repositories specify a citation style (APA).')\n",
    "\n",
    "# Create a stacked bar graph with correct, incorrect, missing fields for each repository\n",
    "\n",
    "## Sort df by most correct fields\n",
    "repo_recommended_citation_sort = repo_recommended_citation.sort_values(['count_correct_ex'], ascending = False)\n",
    "\n",
    "## Start graphing\n",
    "ax = repo_recommended_citation_sort.plot.bar(x = 'Repository', y = ['count_correct_ex','count_missing_ex','count_incorrect_ex'], stacked = True, label = ['Correct', 'Missing','Incorrect'],\n",
    "                                            color = ['tab:blue','tab:orange','tab:red'])\n",
    "plt.ylabel('# Metadata Fields')\n",
    "plt.xticks(rotation = 75)\n",
    "ax.legend(loc = 'lower left',framealpha=1)\n",
    "\n",
    "# need to adjust colors, font sizes, rotation of x labels, graph shape, etc.\n",
    "\n",
    "# Save CSV\n",
    "\n",
    "repo_recommended_citation_sort.iloc[:,[0,5,6,7,8,9,10,11,12,13,14,15]].to_csv(str(filepath+filename+filename2+'.csv')) # Save CSV of df with graph data\n",
    "\n",
    "# Save fig\n",
    "fig.savefig((filepath+filename+filename2+'_vert'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8abbd3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Repository</th>\n",
       "      <th>export-meta-type</th>\n",
       "      <th>export-meta-authors</th>\n",
       "      <th>export-meta-title</th>\n",
       "      <th>export-meta-date</th>\n",
       "      <th>export-meta-DOI</th>\n",
       "      <th>export-meta-repo</th>\n",
       "      <th>export-meta-version</th>\n",
       "      <th>export-meta-access-date</th>\n",
       "      <th>count_incorrect_ex</th>\n",
       "      <th>count_missing_ex</th>\n",
       "      <th>count_correct_ex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Zenodo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Figshare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>IEDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>FDSN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Dryad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>Dataverse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>NCAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>PANGAEA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>EDI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>Mendeley Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>ORNL DAAC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>NASA GES DISC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Climate Data Store</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>PDS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0          Repository  export-meta-type  export-meta-authors  \\\n",
       "0            0              Zenodo               0.0                  0.0   \n",
       "1            1            Figshare               NaN                  0.0   \n",
       "2            3                IEDA               NaN                  0.0   \n",
       "3            6                FDSN               0.0                  0.0   \n",
       "4           10               Dryad               0.0                  0.0   \n",
       "5           11           Dataverse               NaN                  0.0   \n",
       "6            4                NCAR               NaN                  0.0   \n",
       "7            8             PANGAEA               NaN                  0.0   \n",
       "8            9                 EDI               NaN                  0.0   \n",
       "9            7       Mendeley Data               NaN                  1.0   \n",
       "10          12           ORNL DAAC               NaN                  0.0   \n",
       "11           5       NASA GES DISC               NaN                  1.0   \n",
       "12          13  Climate Data Store               NaN                  1.0   \n",
       "13           2                 PDS               NaN                  NaN   \n",
       "\n",
       "    export-meta-title  export-meta-date  export-meta-DOI  export-meta-repo  \\\n",
       "0                 0.0               0.0              0.0               0.0   \n",
       "1                 0.0               0.0              0.0               0.0   \n",
       "2                 0.0               0.0              0.0               0.0   \n",
       "3                 0.0               0.0              0.0               0.0   \n",
       "4                 0.0               0.0              0.0               0.0   \n",
       "5                 0.0               0.0              0.0               0.0   \n",
       "6                 0.0               0.0              0.0               1.0   \n",
       "7                 0.0               0.0              0.0               0.0   \n",
       "8                 0.0               0.0              0.0               0.0   \n",
       "9                 0.0               0.0              0.0               1.0   \n",
       "10                0.0               0.0              0.0               1.0   \n",
       "11                0.0               0.0              0.0               1.0   \n",
       "12                0.0               0.0              0.0               1.0   \n",
       "13                NaN               NaN              NaN               NaN   \n",
       "\n",
       "    export-meta-version  export-meta-access-date  count_incorrect_ex  \\\n",
       "0                   NaN                      NaN                 0.0   \n",
       "1                   0.0                      NaN                 0.0   \n",
       "2                   0.0                      NaN                 0.0   \n",
       "3                   NaN                      NaN                 0.0   \n",
       "4                   NaN                      NaN                 0.0   \n",
       "5                   0.0                      NaN                 0.0   \n",
       "6                   0.0                      NaN                 1.0   \n",
       "7                   NaN                      NaN                 0.0   \n",
       "8                   NaN                      NaN                 0.0   \n",
       "9                   0.0                      NaN                 2.0   \n",
       "10                  NaN                      NaN                 1.0   \n",
       "11                  NaN                      NaN                 2.0   \n",
       "12                  NaN                      NaN                 2.0   \n",
       "13                  NaN                      NaN                 0.0   \n",
       "\n",
       "    count_missing_ex  count_correct_ex  \n",
       "0                  2               6.0  \n",
       "1                  2               6.0  \n",
       "2                  2               6.0  \n",
       "3                  2               6.0  \n",
       "4                  2               6.0  \n",
       "5                  2               6.0  \n",
       "6                  2               5.0  \n",
       "7                  3               5.0  \n",
       "8                  3               5.0  \n",
       "9                  2               4.0  \n",
       "10                 3               4.0  \n",
       "11                 3               3.0  \n",
       "12                 3               3.0  \n",
       "13                 8               0.0  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_recommended_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizonal bar graph of repository recommended citation\n",
    "\n",
    "## Sort df by most correct fields\n",
    "repo_recommended_citation_sort2 = repo_recommended_citation.sort_values(['count_correct_ex'], ascending = True)\n",
    "\n",
    "# Graph\n",
    "ax = repo_recommended_citation_sort2.plot.barh(x = 'Repository', y = ['count_correct_ex','count_missing_ex','count_incorrect_ex'], stacked = True, label = ['Correct', 'Missing','Incorrect'],\n",
    "                                            color = ['tab:blue','tab:orange','tab:red'])\n",
    "plt.xlabel('# Metadata Fields')\n",
    "#ax.legend(loc = 'lower left',framealpha=1)\n",
    "ax.legend(bbox_to_anchor = (1., 1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "fig.savefig((filepath+filename+filename2+'_horiz'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f03e59",
   "metadata": {},
   "source": [
    "# Graph correct fields across all repositories\n",
    "* For each reference manager, what are the # of correct fields across all repositories?\n",
    "\n",
    "This can be done for either the import or export fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b912d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filename\n",
    "\n",
    "filename2 = 'correct-fields-all-repos'\n",
    "\n",
    "# Must exclude crosscite for import\n",
    "# Also exclude bib_desk (import used for something different)\n",
    "\n",
    "list_of_dfs = (zotero_plugin,zotero_wizard,sciwheel_plugin, sciwheel_wizard, \n",
    "             endnote_plugin, endnote_wizard, papers_plugin,papers_wizard,paperpile_plugin,paperpile_wizard,mendeley_plugin,\n",
    "               mendeley_wizard,refworks_plugin,refworks_wizard)\n",
    "\n",
    "# Get list of reference-manager-filename indices to add to repo-import-sums-df\n",
    "list_of_indices = reference_manager_filenames[2:4]+reference_manager_filenames[5:7]+reference_manager_filenames[8:-1]\n",
    "\n",
    "\n",
    "\n",
    "repo_import_sums = sum_across_repos(list_of_dfs, im_cols, 'im')\n",
    "\n",
    "repo_import_sums_df = pd.DataFrame(data = repo_import_sums, index = list_of_indices, columns = ['sum_incorrect', 'sum_missing', 'sum_correct'])\n",
    "repo_import_sums_df_sort = repo_import_sums_df.sort_values(by=['sum_correct'],ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "# Make graph \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(list_of_dfs)) # of reference managers\n",
    "y = 14*8 # Max value possible for repo_export_sums for each ref manager. 14 repos, 8 fields per repo\n",
    "\n",
    "# Get xlabels from index and fix them up\n",
    "xlabels = repo_import_sums_df_sort.index.str.replace('-',' ') # Use labels from repo_import_sums_df_SORT index and remove hyphen\n",
    "xlabels = xlabels.str.replace('wizard','App') # change wizard to App\n",
    "xlabels = xlabels.str.replace('plugin','Plugin') # fix capitalization\n",
    "\n",
    "ylabels = ['0%','25%','50%','75%','100%']\n",
    "\n",
    "offset = 0.25\n",
    "width = 0.25\n",
    "\n",
    "# to get percentages: df[sum_column]/y*100\n",
    "\n",
    "# Graph all\n",
    "ax.bar(x - offset, (repo_import_sums_df_sort['sum_correct']/y*100), width = width, label = 'Correct', color = 'tab:blue')\n",
    "ax.bar(x + offset, (repo_import_sums_df_sort['sum_incorrect']/y*100), width = width, label = 'Incorrect', color = 'tab:red')\n",
    "ax.bar(x, (repo_import_sums_df_sort['sum_missing']/y*100), width = width, label = 'Missing', color = 'tab:orange')\n",
    "\n",
    "# set labels and ticks and axes\n",
    "ax.set_xticks(x,xlabels, rotation = 75) # Either use repo_import_sums_df_sort.index for labels or new string, 'labels'\n",
    "ax.set_yticks([0,25,50,75,100],ylabels) # Either [0,y/4,y/2,3*y/4,y] for raw # metadata fields or [0, 25, 50, 100] for %\n",
    "ax.set_ylabel('% Total Metadata Fields')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save CSV\n",
    "\n",
    "repo_import_sums_df_sort.to_csv(str(filepath+filename+filename2+'.csv')) # Save CSV of df with graph data\n",
    "\n",
    "# Save fig\n",
    "\n",
    "fig.savefig((filepath+filename+filename2+'_vert'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586fd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0aec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal version\n",
    "\n",
    "# Sort ascending = True to put correct values at top\n",
    "repo_import_sums_df_sort = repo_import_sums_df.sort_values(by=['sum_correct'],ascending = True)\n",
    "\n",
    "# Make graph \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(list_of_dfs)) # of reference managers\n",
    "y = 14*8 # Max value possible for repo_export_sums for each ref manager. 14 repos, 8 fields per repo\n",
    "\n",
    "# Get xlabels from index and fix them up\n",
    "xlabels = repo_import_sums_df_sort.index.str.replace('-',' ') # Use labels from repo_import_sums_df_SORT index and remove hyphen\n",
    "xlabels = xlabels.str.replace('wizard','[App]') # change wizard to App\n",
    "xlabels = xlabels.str.replace('plugin','[Plugin]') # fix capitalization\n",
    "\n",
    "ylabels = ['0%','25%','50%','75%','100%']\n",
    "\n",
    "offset = 0.25\n",
    "width = 0.25\n",
    "\n",
    "# to get percentages: df[sum_column]/y*100\n",
    "\n",
    "# Graph all\n",
    "ax.barh(x + offset, (repo_import_sums_df_sort['sum_correct']/y*100), width, label = 'Correct', color = 'tab:blue')\n",
    "ax.barh(x - offset, (repo_import_sums_df_sort['sum_incorrect']/y*100),width, label = 'Incorrect', color = 'tab:red')\n",
    "ax.barh(x, (repo_import_sums_df_sort['sum_missing']/y*100), width,label = 'Missing', color = 'tab:orange')\n",
    "\n",
    "# set labels and ticks and axes\n",
    "ax.set_yticks(x,xlabels, rotation = 0) # Either use repo_import_sums_df_sort.index for labels or new string, 'labels'\n",
    "ax.set_xticks([0,25,50,75,100],ylabels) # Either [0,y/4,y/2,3*y/4,y] for raw # metadata fields or [0, 25, 50, 100] for %\n",
    "ax.set_xlabel('% Total Metadata Fields')\n",
    "ax.legend(bbox_to_anchor=(0.75, 0.6))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "\n",
    "fig.savefig((filepath+filename+filename2+'_horiz'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba3743",
   "metadata": {},
   "source": [
    "# Bibtex\n",
    "* How many repositories offer Bibtex exports and in what formats?\n",
    "* Are Bibtex fields correct? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a17350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First how many repos offer Bibtex exports\n",
    "\n",
    "# filename\n",
    "filename2 = 'bibtex'\n",
    "# Count whether there is a bibtex export or not\n",
    "\n",
    "# no bibtex\n",
    "sum_no_bibtex = np.sum(bibtex['export-exists'].isna())\n",
    "sum_bibtex = len(bibtex['export-exists'])-sum_no_bibtex\n",
    "\n",
    "# Filter by type\n",
    "bibtex_na = bibtex[bibtex['export-exists'].isna()]\n",
    "bibtex_dataset = bibtex[bibtex['export-file-type']=='dataset']\n",
    "bibtex_misc = bibtex[bibtex['export-file-type'].str.contains('misc', na=False)]\n",
    "bibtex_data = bibtex[bibtex['export-file-type']=='data']\n",
    "bibtex_incoll = bibtex[bibtex['export-file-type'].str.contains('incollection', na=False)]\n",
    "bibtex_article = bibtex[bibtex['export-file-type'].str.contains('article', na=False)]\n",
    "\n",
    "\n",
    "# # bar chart for bibtex or NO\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar([1,2], [sum_bibtex, sum_no_bibtex], label = 'Correct')\n",
    "\n",
    "labels = ['No Export','@dataset','@misc','@data','@InCollection','@article']\n",
    "sizes = [len(bibtex_na['export-exists']), len(bibtex_dataset['export-exists']),len(bibtex_misc['export-exists']),\n",
    "         len(bibtex_data['export-exists']),len(bibtex_incoll['export-exists']),len(bibtex_article['export-exists'])]\n",
    "colors = ['tab:red','tab:blue','tab:orange','tab:blue','tab:grey','tab:orange'] # colors corresp. to correctness\n",
    "\n",
    "# Plot pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "patches, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.1f%%', colors = colors)\n",
    "\n",
    "# set inside label parameters: color, size, weight\n",
    "# Color = white\n",
    "for autotext in autotexts: \n",
    "    autotext.set_color('white')\n",
    "\n",
    "# weight and size\n",
    "plt.setp(autotexts, size=11, weight = 'bold')\n",
    "    \n",
    "# show graph\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "\n",
    "fig.savefig((filepath+filename+filename2+'_pie'+'.png'),dpi=300,bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82c35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename\n",
    "\n",
    "filename2 = 'bibtex-fields-by-repo'\n",
    "\n",
    "# Count the correct , incorrect, missing fields - Import Only\n",
    "count_fields(bibtex, im_cols,'im')\n",
    "\n",
    "\n",
    "# Sort fields (filter by only repos offering bibtex export and sort by # correct)\n",
    "bibtex_sort = bibtex.loc[bibtex['export-exists']==0.0].sort_values(by=['count_correct_im'],ascending = False)\n",
    "\n",
    "# Graph\n",
    "ax2 = bibtex_sort.plot.bar(x = 'Repository', y = ['count_correct_im','count_missing_im','count_incorrect_im'], \n",
    "                           stacked = True, \n",
    "                           label = ['Correct', 'Missing','Incorrect'],\n",
    "                          color = ['tab:blue','tab:orange','tab:red'])\n",
    "plt.ylabel('# Metadata Fields')\n",
    "\n",
    "# Get x labels\n",
    "ax2.legend(loc = 'lower left',framealpha=1)\n",
    "ax2.set_xticks(np.arange(7), labels = bibtex_sort['Repository'],rotation=75)\n",
    "plt.show()\n",
    "\n",
    "# Save CSV\n",
    "\n",
    "bibtex_sort.to_csv(str(filepath+filename+filename2+'.csv')) # Save CSV of df with graph data\n",
    "\n",
    "# Save fig\n",
    "fig.savefig((filepath+filename+filename2+'_bar'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8da055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal version\n",
    "\n",
    "# Sort fields (filter by only repos offering bibtex export and sort by # correct)\n",
    "bibtex_sort2 = bibtex.loc[bibtex['export-exists']==0.0].sort_values(by=['count_correct_im'],ascending = True)\n",
    "\n",
    "# Graph\n",
    "ax2 = bibtex_sort2.plot.barh(x = 'Repository', y = ['count_correct_im','count_missing_im','count_incorrect_im'], \n",
    "                           stacked = True, \n",
    "                           label = ['Correct', 'Missing','Incorrect'],\n",
    "                          color = ['tab:blue','tab:orange','tab:red'])\n",
    "plt.xlabel('# Metadata Fields')\n",
    "\n",
    "# Get x labels\n",
    "ax2.legend(bbox_to_anchor = (1., 1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "\n",
    "fig.savefig((filepath+filename+filename2+'_bar_horiz'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05c129",
   "metadata": {},
   "source": [
    "# Best Repository Citation Metadata\n",
    "\n",
    "* are there any repositories that are performing exceptionally well across reference managers?\n",
    "\n",
    "Use import data and sum across sheets - how many '0s'. \n",
    "\n",
    "\n",
    "Could also break out the 8 metadata fields on a by-repository basis and count across the sheets for a single repository for each field, but this would be pretty challenging to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename\n",
    "\n",
    "filename2 = 'repo-metadata-across-refmans'\n",
    "\n",
    "# Must exclude crosscite for import\n",
    "# Bibdesk is also excluded\n",
    "\n",
    "list_of_dfs = (zotero_plugin,zotero_wizard,bib_desk,sciwheel_plugin, sciwheel_wizard, \n",
    "               endnote_wizard, endnote_plugin, papers_plugin,papers_wizard,paperpile_plugin,paperpile_wizard,mendeley_plugin,\n",
    "               mendeley_wizard,refworks_wizard, refworks_plugin)\n",
    "\n",
    "# Get repository metadata field sums across reference managers\n",
    "refman_import_sums = sum_across_refmans(list_of_dfs, im_cols, 'im')\n",
    "\n",
    "\n",
    "# Set labels\n",
    "ylabels = ['0%','25%','50%','75%','100%']\n",
    "\n",
    "# Sort values\n",
    "refman_import_sums_sort = refman_import_sums.sort_values(by=['sum_correct_im'],ascending = False)\n",
    "\n",
    "# Make graph\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(14)\n",
    "\n",
    "y = len(list_of_dfs)*8 # Max value possible for refman_sums for each ref manager\n",
    "\n",
    "# Percents are df[sum_column]/y*100\n",
    "\n",
    "offset = 0.25\n",
    "width = 0.25\n",
    "ax.bar(x - offset, (refman_import_sums_sort['sum_correct_im']/y*100), width = width, label = 'Correct', color = 'tab:blue')\n",
    "ax.bar(x + offset, (refman_import_sums_sort['sum_incorrect_im']/y*100), width = width, label = 'Incorrect', color = 'tab:red')\n",
    "ax.bar(x, (refman_import_sums_sort['sum_missing_im']/y*100), width = width, label = 'Missing', color = 'tab:orange')\n",
    "\n",
    "# set labels and ticks and axes\n",
    "ax.set_xticks(x,refman_import_sums_sort['Repository'], rotation = 75) # Use Repository column\n",
    "ax.set_yticks([0,(y/y)/4*100,(y/y)/2*100,3*(y/y)/4*100,y/y*100],ylabels)\n",
    "ax.set_ylabel('% Total Metadata Fields')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save CSV\n",
    "\n",
    "refman_import_sums.to_csv(str(filepath+filename+filename2+'.csv')) # Save CSV of df with graph data\n",
    "\n",
    "# Save fig\n",
    "fig.savefig((filepath+filename+filename2+'_vert'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd7528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal version\n",
    "\n",
    "# Sort values\n",
    "refman_import_sums_sort2 = refman_import_sums.sort_values(by=['sum_correct_im'],ascending = True)\n",
    "\n",
    "# Make graph\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(14)\n",
    "\n",
    "y = len(list_of_dfs)*8 # Max value possible for refman_sums for each ref manager\n",
    "\n",
    "# Percents are df[sum_column]/y*100\n",
    "\n",
    "offset = 0.25\n",
    "width = 0.25\n",
    "ax.barh(x + offset, (refman_import_sums_sort2['sum_correct_im']/y*100), width, label = 'Correct', color = 'tab:blue')\n",
    "ax.barh(x - offset, (refman_import_sums_sort2['sum_incorrect_im']/y*100), width, label = 'Incorrect', color = 'tab:red')\n",
    "ax.barh(x, (refman_import_sums_sort2['sum_missing_im']/y*100), width, label = 'Missing', color = 'tab:orange')\n",
    "\n",
    "# set labels and ticks and axes\n",
    "ax.set_yticks(x,refman_import_sums_sort2['Repository']) # Use Repository column\n",
    "ax.set_xticks([0,(y/y)/4*100,(y/y)/2*100,3*(y/y)/4*100,y/y*100],ylabels)\n",
    "ax.set_xlabel('% Total Metadata Fields')\n",
    "ax.legend(bbox_to_anchor = (1.,1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "\n",
    "fig.savefig((filepath+filename+filename2+'_horiz'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec9dca",
   "metadata": {},
   "source": [
    "# Most common errors by metadata field\n",
    "\n",
    "* by import\n",
    "* by export\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11eda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filename\n",
    "\n",
    "filename2 = 'errors-by-field'\n",
    "\n",
    "# list dfs to include\n",
    "\n",
    "# should NOT include cross cite? \n",
    "# should ALSO not include BibDesk as import type field is used for something else\n",
    "\n",
    "list_of_dfs = (zotero_plugin,zotero_wizard,sciwheel_plugin, sciwheel_wizard, endnote_wizard, endnote_plugin, \n",
    "               papers_plugin,papers_wizard,paperpile_plugin,paperpile_wizard,mendeley_plugin,\n",
    "               mendeley_wizard,refworks_wizard, refworks_plugin)\n",
    "\n",
    "# Get df with sum incorrect, missing, and correct for each metadata field by import and export\n",
    "\n",
    "store_data = sum_across_fields(list_of_dfs)\n",
    "\n",
    "# For both export and import, graph as a stack correct, incorrect, missing for each metadata field\n",
    "\n",
    "# Common parameters for graphing\n",
    "\n",
    "x = np.arange(8) # of metadata fields\n",
    "y = len(list_of_dfs)*len(zotero_plugin['Repository']) # total possible correct fields, len list of dfs * 14 repositories\n",
    "\n",
    "## Start graphing IMPORT\n",
    "\n",
    "# first sort import\n",
    "import_sort = store_data.loc['import-meta-DOI':'import-meta-version'].sort_values(by = 'sum_correct',ascending = False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "offset = 0.25\n",
    "width = 0.25\n",
    "ax.bar(x - offset, (import_sort['sum_correct']/y*100), width = width, label = 'Correct', color = 'tab:blue')\n",
    "ax.bar(x + offset, (import_sort['sum_incorrect']/y*100), width = width, label = 'Incorrect', color = 'tab:red')\n",
    "ax.bar(x, (import_sort['sum_missing']/y*100), width = width, label = 'Missing', color = 'tab:orange')\n",
    "\n",
    "# Get labels and clean them up\n",
    "xlabels = import_sort.index #labels will be import_sort.index Prev store_data index DOI thru version. Import is 16:24\n",
    "xlabels = xlabels.str.replace('-',' ') # replace hyphen\n",
    "xlabels = xlabels.str.replace('import','') #don't include 'import'\n",
    "xlabels = xlabels.str.replace('meta','') #don't include 'meta'\n",
    "xlabels = xlabels.str.replace('repo','repository') \n",
    "\n",
    "# set labels and ticks and axes\n",
    "\n",
    "ax.set_xticks(x,xlabels, rotation = 75) # Use xlabels or index\n",
    "ax.set_yticks([0,(y/y)/4*100,(y/y)/2*100,3*(y/y)/4*100,y/y*100],ylabels)\n",
    "ax.set_ylabel('% Total Metadata Fields')\n",
    "ax.legend()\n",
    "plt.title('Import')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "\n",
    "fig.savefig((filepath+filename+filename2+'_import_vert'+'.png'),dpi=300,bbox_inches='tight')\n",
    "\n",
    "## Start graphing EXPORT \n",
    "\n",
    "# first sort export\n",
    "export_sort = store_data.loc['export-meta-DOI':'export-meta-version'].sort_values(by = 'sum_correct',ascending = False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "offset = 0.25\n",
    "width = 0.25\n",
    "ax.bar(x - offset, (export_sort['sum_correct']/y*100), width = width, label = 'Correct', color = 'tab:blue')\n",
    "ax.bar(x + offset, (export_sort['sum_incorrect']/y*100), width = width, label = 'Incorrect', color = 'tab:red')\n",
    "ax.bar(x, (export_sort['sum_missing']/y*100), width = width, label = 'Missing', color = 'tab:orange')\n",
    "\n",
    "# Get labels and clean them up\n",
    "xlabels = export_sort.index #labels will be export_sort index Prev store_data index DOI thru version. Export is 7:15\n",
    "xlabels = xlabels.str.replace('-',' ') # replace hyphen\n",
    "xlabels = xlabels.str.replace('export','') #don't include 'import'\n",
    "xlabels = xlabels.str.replace('meta','') #don't include 'meta'\n",
    "xlabels = xlabels.str.replace('repo','repository') \n",
    "\n",
    "\n",
    "# set labels and ticks and axes\n",
    "ax.set_xticks(x,xlabels, rotation = 75) # Use xlabels or index\n",
    "ax.set_yticks([0,(y/y)/4*100,(y/y)/2*100,3*(y/y)/4*100,y/y*100],ylabels)\n",
    "ax.set_ylabel('% Total Metadata Fields')\n",
    "ax.legend()\n",
    "plt.title('Export')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "\n",
    "fig.savefig((filepath+filename+filename2+'_export_vert'+'.png'),dpi=300,bbox_inches='tight')\n",
    "\n",
    "# Save CSV for both\n",
    "\n",
    "store_data.to_csv(str(filepath+filename+filename2+'.csv')) # Save CSV of df with graph data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal version\n",
    "\n",
    "# Common parameters for graphing\n",
    "\n",
    "x = np.arange(8) # of metadata fields\n",
    "y = len(list_of_dfs)*len(zotero_plugin['Repository']) # total possible correct fields, len list of dfs * 14 repositories\n",
    "\n",
    "## Start graphing IMPORT \n",
    "\n",
    "# first sort import\n",
    "import_sort = store_data.loc['import-meta-DOI':'import-meta-version'].sort_values(by = 'sum_correct',ascending = True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "offset = 0.25\n",
    "width = 0.25\n",
    "ax.barh(x + offset, import_sort['sum_correct']/y*100, width, label = 'Correct', color = 'tab:blue')\n",
    "ax.barh(x - offset, import_sort['sum_incorrect']/y*100, width, label = 'Incorrect', color = 'tab:red')\n",
    "ax.barh(x, import_sort['sum_missing']/y*100, width, label = 'Missing', color = 'tab:orange')\n",
    "\n",
    "# Get labels and clean them up\n",
    "xlabels = import_sort.index #labels will be import_sort index DOI thru version. Prev used store_data index, import is 16:24\n",
    "xlabels = xlabels.str.replace('-',' ') # replace hyphen\n",
    "xlabels = xlabels.str.replace('import','') #don't include 'import'\n",
    "xlabels = xlabels.str.replace('meta','') #don't include 'meta'\n",
    "xlabels = xlabels.str.replace('repo','repository') \n",
    "\n",
    "# set labels and ticks and axes\n",
    "ax.set_yticks(x,xlabels) # Use xlabels or index\n",
    "ax.set_xticks([0,(y/y)/4*100,(y/y)/2*100,3*(y/y)/4*100,y/y*100],ylabels)\n",
    "ax.set_xlabel('% Total Metadata Fields')\n",
    "ax.legend()\n",
    "plt.title('Import')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "\n",
    "fig.savefig((filepath+filename+filename2+'_import_horiz'+'.png'),dpi=300,bbox_inches='tight')\n",
    "\n",
    "\n",
    "## Start graphing EXPORT \n",
    "\n",
    "# first sort export\n",
    "export_sort = store_data.loc['export-meta-DOI':'export-meta-version'].sort_values(by = 'sum_correct',ascending = True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "offset = 0.25\n",
    "width = 0.25\n",
    "ax.barh(x + offset, (export_sort['sum_correct']/y*100), width, label = 'Correct', color = 'tab:blue')\n",
    "ax.barh(x - offset, (export_sort['sum_incorrect']/y*100), width, label = 'Incorrect', color = 'tab:red')\n",
    "ax.barh(x, (export_sort['sum_missing']/y*100), width, label = 'Missing', color = 'tab:orange')\n",
    "\n",
    "# Get labels and clean them up\n",
    "xlabels = export_sort.index #labels will be export_sort index DOI thru version. Prev used store_data index, export\n",
    "xlabels = xlabels.str.replace('-',' ') # replace hyphen\n",
    "xlabels = xlabels.str.replace('export','') #don't include 'import'\n",
    "xlabels = xlabels.str.replace('meta','') #don't include 'meta'\n",
    "xlabels = xlabels.str.replace('repo','repository') \n",
    "\n",
    "# set labels and ticks and axes\n",
    "ax.set_yticks(x,xlabels) # Use xlabels or index\n",
    "ax.set_xticks([0,(y/y)/4*100,(y/y)/2*100,3*(y/y)/4*100,y/y*100],ylabels)\n",
    "ax.set_xlabel('% Total Metadata Fields')\n",
    "ax.legend()\n",
    "plt.title('Export')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save fig\n",
    "fig.savefig((filepath+filename+filename2+'_export_horiz'+'.png'),dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b1c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0dd06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e2d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e9723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eaa201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e346767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d30c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bda9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58743b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aaae19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03633f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57bc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
